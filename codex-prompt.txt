Role: You are Codex working locally in VS Code on Windows. Do NOT git commit or git push.

Context:
- Step A is complete. It outputs:
  - <stepA_out>/reduced_pool/  (primary survivor images only)
  - <stepA_out>/step_a_manifest.jsonl (group-level)
- Step B should ONLY use the reduced_pool primaries (ignore alternates).
- User style tags are a fixed set of 4 moods; user can pick up to 2:
  1) Classic & Timeless
  2) Lively & Spontaneous
  3) Artistic Eye
  4) Elegant Portrait
- Step B ends after producing:
  - DINO k-means clusters (visual grouping)
  - CLIP naming of each cluster
  - CLIP-based ranking of images within each cluster using cosine similarity to selected user styles
- No Step C logic in this prompt.

High-level Step B requirements:
1) Compute and cache embeddings:
   - DINO image embeddings (for clustering)
   - CLIP image embeddings (for text alignment)
   - CLIP text embeddings for the 4 fixed tags (or the 1–2 user-selected subset)
   Cache so reruns do not recompute unless forced.

2) Cluster:
   - Run k-means on DINO embeddings.
   - Deterministic: set random_state; set n_init to an explicit int (do not rely on sklearn 'auto').

3) Name clusters + rank within clusters:
   - For each image, compute CLIP similarity to each selected style text embedding:
       score(style_i, image) = cosine(clip_img_vec, clip_text_vec_i)
   - pref_score = max(score over selected styles)
   - Within each DINO cluster, rank images by:
       pref_score desc, then Step A quality (if available), then path asc
     Note: Step A quality is optional; if you can read per-image quality from quality.jsonl from earlier, use it; otherwise skip.
   - Cluster label: aggregate CLIP style scores across images in cluster (e.g. mean pref_score per style), pick top style label as cluster_name.
     Also store a cluster_pref_score for cluster ordering (e.g. mean of top-5 pref_scores).

Outputs (artifacts):
- <stepB_out>/step_b_images.jsonl  (one record per image)
- <stepB_out>/step_b_clusters.jsonl (one record per cluster)
- <stepB_out>/cache/
    - dino_embeddings.npy (or .npz) plus an index mapping to paths
    - clip_image_embeddings.npy
    - clip_text_embeddings.npy (for the 4 tags and prompt variants)
- Optional debug: <stepB_out>/debug_kmeans_centroids.npy

JSONL schemas:
A) step_b_images.jsonl:
   {
     "path": "relative/path/from_stepA_out/reduced_pool/...",
     "cluster_id": int,
     "styles_topk": [{"tag": str, "score": float}, ... up to k],
     "pref_score": float,
     "rank_in_cluster": int,
     "quality": { ...optional subset... } | null
   }

B) step_b_clusters.jsonl:
   {
     "cluster_id": int,
     "size": int,
     "cluster_name": str,                # one of the 4 fixed tags
     "cluster_style_scores": [{"tag": str, "score": float}, ...],  # aggregate scores
     "cluster_pref_score": float,
     "representatives": [ "path1", "path2", ... up to 6 ]          # top ranked paths
   }

Determinism:
- Stable input order: sort paths.
- Cache index maps path->row index deterministically.
- KMeans: set random_state=0 and n_init=10 (or explicit int) so repeated runs are identical for same embeddings.
  Avoid n_init='auto' to prevent version-dependent behavior.
- Sorting tie-breaks always include path asc.

Dependencies and constraints:
- Keep diffs minimal; avoid refactoring unrelated modules.
- Use widely available libs:
  - torch + transformers for DINOv2 (Hugging Face Dinov2Model)
  - open_clip for CLIP embeddings OR use existing CLIP code if already present
  - scikit-learn for KMeans
  - numpy
- If these deps are not installed in the repo, implement as:
  - A clear ImportError message that tells user what to pip install
  - Do not silently fail
- Keep GPU optional; should run on CPU by default (slow is fine for MVP).

Implementation steps:
1) Inspect repo:
   - Find Step A runner and confirm it creates reduced_pool and manifest.
   - Confirm existing dependency management (pyproject/requirements).
   - Confirm whether torch/transformers/open_clip/sklearn already exist.
2) Add new Step B runner:
   - CREATE: labs/step_b/runner.py
   - CREATE: labs/step_b/__init__.py (empty)
3) Step B CLI:
   python -m labs.step_b.runner
     --step-a-out <path>                  # Step A output folder that contains reduced_pool
     --out <stepB_out_path>
     --k <int>                            # k-means clusters; default heuristic if omitted
     --styles "Classic & Timeless|Artistic Eye"   # up to 2 tags separated by '|'
     --clip-model <name>                  # default to a small OpenCLIP checkpoint
     --dino-model <name>                  # default to dinov2-base or dinov2-small
     --batch-size <int>                   # default 8 (CPU)
     --recompute                           # ignore cache and recompute embeddings
     --print-summary
     --debug

   k default heuristic:
     - if k not provided: k = min(24, max(6, round(sqrt(n_images))))
   Ensure k <= n_images.

4) Embedding extraction:
   - Resolve image paths from <step-a-out>/reduced_pool.
   - DINO embedding:
     - Use transformers Dinov2Model; take CLS token from last_hidden_state[:,0,:] as embedding.
       (DINOv2 returns class token and patch tokens; CLS is global vector.)
   - CLIP embedding:
     - Use open_clip to encode images and text in the same space; normalize vectors for cosine.
     - Build text prompts for the 4 tags:
       Use 2 templates per tag for stability, e.g.:
         "a travel photo with a {tag} vibe"
         "{tag} travel photography aesthetic"
       Then average the two text embeddings per tag.
   - Cache both embeddings and a paths index file (json).

5) Clustering:
   - KMeans on DINO embeddings:
     - sklearn.cluster.KMeans(n_clusters=k, random_state=0, n_init=10, init='k-means++')
   - Save cluster_id per image.

6) CLIP scoring + naming + ranking:
   - Parse user styles list (1–2 of the 4 tags).
   - For each image:
       compute score per selected style using dot product of normalized vectors
       pref_score = max selected
       also compute topk across all 4 tags for display (k=4 is fine)
   - For each cluster:
       aggregate style scores across images (mean per tag)
       cluster_name = argmax aggregated score (over the 4 tags)
       cluster_pref_score = mean of top-5 pref_scores in that cluster (or mean of all if <5)
   - Ranking within cluster:
       sort by pref_score desc, then optional quality, then path asc
     For quality, if you can locate a quality.jsonl produced in Step A run folder, read sharp_vlap and use it as tie-break (desc).

7) Write JSONL artifacts exactly as specified above, using relative paths from step-a-out for portability.
8) Acceptance checks (run and report):
   A) Run on a folder with at least 30 images:
      - confirm cache files exist
      - confirm step_b_images.jsonl line count equals #images
      - confirm step_b_clusters.jsonl line count equals k
   B) Determinism:
      - run twice with same inputs and cache, ensure outputs identical
      - run twice with --recompute, ensure outputs identical (CPU determinism)
   C) Sanity:
      - print cluster sizes summary and top 3 reps per cluster.

Deliverables:
- New files created
- Commands run for A/B/C
- First 2 lines of each JSONL artifact
- Short notes: what models/checkpoints used by default

Notes:
- KMeans parameter behavior: avoid n_init='auto' for stability. (sklearn docs)
- CLIP zero-shot is cosine similarity between image/text embeddings. (CLIP docs / references)
- DINOv2 returns class token + patch tokens; use CLS token as global embedding.